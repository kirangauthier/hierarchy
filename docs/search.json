[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Welcome to hierarchy, a blog about using regression, causal inference, machine learning, and hierarchical (multilevel) modeling to distinguish signal from noise. More broadly, we’re going to use computational statistics to:\n\nmeasure unknowns\nthink generatively\nquantify “luck” and “surprise”\nvisualize and communicate the implications of our model & inference\n\nI (Kiran) did my PhD in the Bishop Group at Columbia University, graduating in August 2022, where I defended my thesis on optimal experimental design for hierarchical, nonlinear systems to characterize and design autonomous behaviours in microrobots. You can find it here. My goal was to use external fields (think magnetic, acoustic) to encode behaviours into microscale colloids to help them autonomously sense their environment, and accomplish tasks in noisy environments, like suppressing dendrite formation in lithium ion batteries, swimming against the flow to deliver cargo, and sniffing your way through stochastic chemical gradients.\nThese are all part of the larger subset of distinguishing signal from noise, and the statistical models I worked on can be applied to autonomous driving, predicting venture captial risk, and selecting optimal dosing regimens for rare diseases, among many other applications. We’ll use tools from probability and information theory, and leverage modern compute and other tricks to answer these problems, refining our models and understanding as we go."
  },
  {
    "objectID": "about.html#about-hierarchy",
    "href": "about.html#about-hierarchy",
    "title": "About me",
    "section": "",
    "text": "Welcome to hierarchy, a blog about using regression, causal inference, machine learning, and hierarchical (multilevel) modeling to distinguish signal from noise. More broadly, we’re going to use computational statistics to:\n\nmeasure unknowns\nthink generatively\nquantify “luck” and “surprise”\nvisualize and communicate the implications of our model & inference\n\nI (Kiran) did my PhD in the Bishop Group at Columbia University, graduating in August 2022, where I defended my thesis on optimal experimental design for hierarchical, nonlinear systems to characterize and design autonomous behaviours in microrobots. You can find it here. My goal was to use external fields (think magnetic, acoustic) to encode behaviours into microscale colloids to help them autonomously sense their environment, and accomplish tasks in noisy environments, like suppressing dendrite formation in lithium ion batteries, swimming against the flow to deliver cargo, and sniffing your way through stochastic chemical gradients.\nThese are all part of the larger subset of distinguishing signal from noise, and the statistical models I worked on can be applied to autonomous driving, predicting venture captial risk, and selecting optimal dosing regimens for rare diseases, among many other applications. We’ll use tools from probability and information theory, and leverage modern compute and other tricks to answer these problems, refining our models and understanding as we go."
  },
  {
    "objectID": "about.html#some-initial-thoughts",
    "href": "about.html#some-initial-thoughts",
    "title": "About me",
    "section": "Some initial thoughts",
    "text": "Some initial thoughts\n\nPortage\nstealing from astronomy astronomy (not astrology, I checked), psychology, psychometrics, ecology, econometrics,\nadd the photo in here\n\n\nData quality is paramount\nI started thinking about the value of data early on in grad school, because my experiments took a long time to run and a lot of them gave me useless results. As an added complication, my setup varied slightly every day, so I had to constantly recalibrate my understanding of the underlying parameters of my system. That led me to thinking about experimental design to suggest maximally informative experiments conditional on the data that I’ve already observed. I highly recommend this excellent paper from Tom Loredo which got me started about thinking in terms of probabilities and distributions, and how we quantify information using entropy. See the section on experimental design for more info.\n\n\nWhy do I believe in statistical modeling?\nChris Rackauckas said in a talk that “the major advances in machine learning were due to encoding more structure into the model” with a subquote of “more structure = faster and better fits from data” and that sums it up pretty well. I think that building models that more closely resemble the “true” data generating process are our best hope of actually learning what’s going on under the hood. And although it’s fun to throw compute at a problem, I believe in failing fast and in the power of the iterative model building workflow often using coarse-grained models and approximate inference.\nThis perspective was perfectly summarized by George Box when he said that “all models are wrong, but some are useful”. If our models are only ever an approximation of reality, and our compute, and mental sanity are finite, then it’s worth it to see what we can get away with before we build out luxurious models.\nOur goal is then to find a model, \\(M\\), that is that is an efficient coarse-grained of reality,\n\n\n\nModels (\\(p(y \\mid \\theta, M)\\)) approximate the true relationship between design and data \\(q(y \\mid d)\\).\n\n\nThe framework that I developed is especially useful when you have a small amount of data, observed from noisy individuals in heterogeneous groups (hierarchy), and you need to find where to look next (design). Think “how much medicine should I treat a group of patients with rare disease given that they will all process the drug in distinctively different ways”? people are inherently different?\nBut I firmly believe that not all data is created equal, and focusing on acquiring high quality data that maximize the predictive power of our models will be the next revolution in science If you put bad data into a AI or ML algorithm it’ll fail without warning with potentially devastating consequences I’m sure you can empathize this with your work"
  },
  {
    "objectID": "about.html#any-qualms",
    "href": "about.html#any-qualms",
    "title": "About me",
    "section": "Any qualms",
    "text": "Any qualms\n,\nmultimodality\nfolk theorem of statistical computation\nintersection of what cna be done and what should be done, is the juice worth the squeeze\ndistinguish signal from noise\nhierarcchal models are everywhere\nExperimental design Other great papers to use as introductory material to experimental design are Huan and Marzouk’s paper, Dennis Lindley’s entire catalog, but this paper in particular, and Andrew Gelman’s entire catalog, footnotes, and this paper.\nWhy his footnotes? I can’t find the quote now (if you remember it, email me!) but it went something along the lines of “every original thought you’ve ever had about statistics has already been published by Andrew [Gelman] in a footnote of a paper from the 1970s”.\nI’m Kiran and I’m a computational statistician, graduating with my PhD from Columbia University other topics to figure out if the juice is worth the squeeze. I’ll be programming in Python, R, Stan, and Julia, and am a huge advocate of failing fast, model parsimony, and Occam’s razor, so we’ll be timing, multiprocessing, broadcasting, and using whichever backend can get us a close-enough answer fastest.\nI did my PhD in the Bishop group ………………………… I wrote my dissertation using PyMC3 (now PyMC), and have recently switched to Stan because I’ve been writing in R a lot more. This blog will also give me a chance to revisit PyMC, Pyro and NumPyro (built on top of PyTorch), TFP (built on top of TensorFlow), and Turing.jl to see which is fast and readable.\nI also think that visualization is a huge part of my workflow, which is why I’ve been super impressed with the ease-of-use of the brms package (not to mention Paul’s support on GitHub and the Stan Discourse), because getting inferential models to act generatively is pretty clunky and requires custom code to develop good visuals. brms takes care of this for you. To my knowledge, there’s no Python equivalent so we’ll flip back and forth depending on how far we are in the prototyping stage.\nMy favourite book right now is Richard McElreath’s “Statistical Rethinking”, and his accompanying lecture series which are both incredibly well written / produced, and are great discussions of challenges with asking the “oracle” of regression seemingly innocuous questions (see the height ~ +\n\n\n\nmodel for a great example).\nI believe that there’s huge value in modeling data hierarchially, where groups of individuals are drawn from a population and we’d like to have information flow between them, but these models are often slow, hard to fit, and are usually much harder to set up than conventional non-hierarchical models. I’m hoping to have a repository that acts as a bridge between the different languages, and packages, so that we can all benefit from the massive amount of documentation and knowledge that has already been shared.\nI may make errors, if you see one, please email kiran.gauthier@columbia.edu me or leave a comment and I’ll revise it!\nLibrary I’ll fill this later.\nOther blogs I read Gelman Betancourt Frank Harrell Aki Vehtari\nPosts to come BridgeStan Simpson’s paradox Bonferroni correction Talking about interactions\nWish list item: see how information flows between parameters, include something about synthetic data generation.\nAdd draft: true to the document options if you’d like a post to not be included in the listing, site map, or site search. For example: #— title: “My Post” description: “Post description” author: “Fizz McPhee” date: “5/22/2021” draft: true #—"
  },
  {
    "objectID": "posts/why-hierarchy/index.html",
    "href": "posts/why-hierarchy/index.html",
    "title": "Why hierarchical models?",
    "section": "",
    "text": "Things that look like other things can share information. This is the fundamental concept that underlies hierarchical modeling. … and it has the nice property of helping to stabilize our estimates, especially when there’s not a ton of data.\nIntuitively, this makes sense, if I know something about your neighbor, it might tell me something about you. If I treated you as independent entities, I’m probably throwing away information. There is a chance that you’re very dissimilar, but odds are you are more similar to people in your county / state / who share your same job / play the same position as you than you are for people in other groups."
  },
  {
    "objectID": "posts/why-hierarchy/index.html#why-is-hierarchy-important",
    "href": "posts/why-hierarchy/index.html#why-is-hierarchy-important",
    "title": "Why hierarchical models?",
    "section": "",
    "text": "Things that look like other things can share information. This is the fundamental concept that underlies hierarchical modeling. … and it has the nice property of helping to stabilize our estimates, especially when there’s not a ton of data.\nIntuitively, this makes sense, if I know something about your neighbor, it might tell me something about you. If I treated you as independent entities, I’m probably throwing away information. There is a chance that you’re very dissimilar, but odds are you are more similar to people in your county / state / who share your same job / play the same position as you than you are for people in other groups."
  },
  {
    "objectID": "posts/why-hierarchy/index.html#modeling-home-runs",
    "href": "posts/why-hierarchy/index.html#modeling-home-runs",
    "title": "Why hierarchical models?",
    "section": "Modeling home runs",
    "text": "Modeling home runs\n\nPlayers as independent entities\nSay we’re trying to model home rates of home runs in the MLB, we could use a binomial distribution to estimate the probability of hitting a home run \\(\\theta\\) for each player in the league \\(1, \\dots, k\\), let’s assume it’s fixed for the given period we observe these players. Your likelihood for the \\(k^\\text{th}\\) player with \\(n\\) at bats and \\(h\\) home runs would look something like1:\n\\[\np(h \\mid n, \\theta) = {n \\choose h} \\theta^h (1 - \\theta)^{n-h}\n\\]\nor, graphically,\n![]\nIntuititvely, this should make sense, the odds of you hitting a home run at any given at bat are \\(\\theta\\). Hitting \\(h\\) home runs given \\(n\\) trials means you had \\(h\\) trials with probability \\(\\theta\\) and \\(n-h\\) trials where you didn’t, which must be realizations of the probability \\(1 - \\theta\\). So far we’re at \\(\\theta^h (1 - \\theta)^{n-h}\\). Taking care of some ordering with the \\({n}\\choose{h}\\), we account for the many ways that we could have had \\(h\\) home runs in \\(n\\) trials, and we end up with a normalized (this just means \\(\\int p(\\mathbf{x}) d\\mathbf{x} = 1\\)) probability density which has a mean value of \\(\\theta n\\) and a variance of \\(n \\theta (1 - \\theta)\\).\nThis should also make sense, given \\(n\\) trials, and an independent probability of success, \\(\\theta\\), we’d expect that the mean number of home runs is \\(\\mathcal{E}[h] = n \\theta\\). What’s interesting, however, is that our variance grows linearly with the number of at bats, as \\(\\text{var}(h) = n \\theta (1 - \\theta) \\propto n\\). So as we have more at bats, the more uncertain we are in the total number of home runs.\nadd the figures here… cite them too\ncomplication is low at bats\nThis is more formally known as partial pooling, summarized well here, and this\nwhat’s the role of priors?\nsimpson’s paradox\nwhy are hierarchical models hard to fit?\npitchers look like other ptichers (not Shohei)\n\\[\np(h_k \\mid n_k, \\theta_k) = {n_k \\choose h_k} \\theta_k^{h_k} (1 - \\theta_k)^{n_k - h_k}\n\\]\nbut I like to drop indexes where I can to simplify notation."
  },
  {
    "objectID": "posts/why-hierarchy/index.html#comments-add-to-blog",
    "href": "posts/why-hierarchy/index.html#comments-add-to-blog",
    "title": "Why hierarchical models?",
    "section": "comments add to blog??",
    "text": "comments add to blog??"
  },
  {
    "objectID": "posts/why-hierarchy/index.html#footnotes",
    "href": "posts/why-hierarchy/index.html#footnotes",
    "title": "Why hierarchical models?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, we should have written the likelihood over the players as:↩︎"
  },
  {
    "objectID": "posts/3-card-riddle/index.html",
    "href": "posts/3-card-riddle/index.html",
    "title": "Why hierarchical models?",
    "section": "",
    "text": "Things that look like other things can share information. This is the fundamental underlying hierarchical modeling, and it has the nice property of helping to stabilize our estimates, especially when there’s not a ton of data.\nIntuitively, this makes sense, if I know something about your neighbor, it might tell me something about you. If I treated you as independent entities, I’m probably throwing away information. There is a chance that you’re very dissimilar, but odds are you are more similar to people in your county / state / who share your same job / play the same position as you than you are for people in other groups."
  },
  {
    "objectID": "posts/3-card-riddle/index.html#why-is-hierarchy-important",
    "href": "posts/3-card-riddle/index.html#why-is-hierarchy-important",
    "title": "Why hierarchical models?",
    "section": "",
    "text": "Things that look like other things can share information. This is the fundamental underlying hierarchical modeling, and it has the nice property of helping to stabilize our estimates, especially when there’s not a ton of data.\nIntuitively, this makes sense, if I know something about your neighbor, it might tell me something about you. If I treated you as independent entities, I’m probably throwing away information. There is a chance that you’re very dissimilar, but odds are you are more similar to people in your county / state / who share your same job / play the same position as you than you are for people in other groups."
  },
  {
    "objectID": "posts/3-card-riddle/index.html#modeling-home-runs",
    "href": "posts/3-card-riddle/index.html#modeling-home-runs",
    "title": "Why hierarchical models?",
    "section": "Modeling home runs",
    "text": "Modeling home runs\n\nPlayers as independent entities\nSay we’re trying to model home rates of home runs in the MLB, we could use a binomial distribution to estimate the probability of hitting a home run \\(\\theta\\) for each player in the league \\(1, \\dots, k\\), let’s assume it’s fixed for the given period we observe these players. Your likelihood for the \\(k^\\text{th}\\) player with \\(n\\) at bats and \\(h\\) home runs would look something like1:\n\\[\np(h \\mid n, \\theta) = {n \\choose h} \\theta^h (1 - \\theta)^{n-h}\n\\]\nor, graphically,\n![]\nIntuititvely, this should make sense, the odds of you hitting a home run at any given at bat are \\(\\theta\\). Hitting \\(h\\) home runs given \\(n\\) trials means you had \\(h\\) trials with probability \\(\\theta\\) and \\(n-h\\) trials where you didn’t, which must be realizations of the probability \\(1 - \\theta\\). So far we’re at \\(\\theta^h (1 - \\theta)^{n-h}\\). Taking care of some ordering with the \\({n}\\choose{h}\\), we account for the many ways that we could have had \\(h\\) home runs in \\(n\\) trials, and we end up with a normalized (this just means \\(\\int p(\\mathbf{x}) d\\mathbf{x} = 1\\)) probability density which has a mean value of \\(\\theta n\\) and a variance of \\(n \\theta (1 - \\theta)\\).\nThis should also make sense, given \\(n\\) trials, and an independent probability of success, \\(\\theta\\), we’d expect that the mean number of home runs is \\(\\mathcal{E}[h] = n \\theta\\). What’s interesting, however, is that our variance grows linearly with the number of at bats, as \\(\\text{var}(h) = n \\theta (1 - \\theta) \\propto n\\). So as we have more at bats, the more uncertain we are in the total number of home runs.\nadd the figures here… cite them too\ncomplication is low at bats\nThis is more formally known as partial pooling, summarized well here, and this\nwhat’s the role of priors?\nsimpson’s paradox\nwhy are hierarchical models hard to fit?\npitchers look like other ptichers (not Shohei)\n\\[\np(h_k \\mid n_k, \\theta_k) = {n_k \\choose h_k} \\theta_k^{h_k} (1 - \\theta_k)^{n_k - h_k}\n\\]\nbut I like to drop indexes where I can to simplify notation."
  },
  {
    "objectID": "posts/3-card-riddle/index.html#comments-add-to-blog",
    "href": "posts/3-card-riddle/index.html#comments-add-to-blog",
    "title": "Why hierarchical models?",
    "section": "comments add to blog??",
    "text": "comments add to blog??"
  },
  {
    "objectID": "posts/3-card-riddle/index.html#footnotes",
    "href": "posts/3-card-riddle/index.html#footnotes",
    "title": "Why hierarchical models?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, we should have written the likelihood over the players as:↩︎"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/notin/index.html",
    "href": "posts/notin/index.html",
    "title": "%notin%",
    "section": "",
    "text": "I find the base %in% operator to be super useful to keep code readable in R, but sometimes I want everything but what I specify in my dplyr::filter call.\nThankfully, there’s a really nice way to do it by making use of the Negate() function. Here’s the code to run it.\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n})\n\n`%notin%` &lt;- Negate(`%in%`)\n\n\n\nLet’s try it out!\n\nsuppressPackageStartupMessages({\n  library(magrittr)\n\n  library(palmerpenguins)\n})\ndata(package = 'palmerpenguins')\n\n## get the head of the df\npenguins %&gt;%\n  head()\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n## get the first three colors from the Tableau10 palette\ntab10_colors &lt;- c(\"#4E79A7\", \"#F28E2B\", \"#E15759\")\n\n\n## print the unique islands\npenguins %$%\n  unique(island)\n\n[1] Torgersen Biscoe    Dream    \nLevels: Biscoe Dream Torgersen\n\n\n\n## check the %in% operator\npenguins %&gt;%\n\n  filter(island %in% c('Biscoe', 'Torgersen')) %&gt;%\n\n  select(species, island, bill_length_mm, bill_depth_mm, year) %&gt;%\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = island)) +\n  geom_point(size=3) +\n  scale_color_manual(values = c(\"Biscoe\" = tab10_colors[1], \"Torgersen\" = tab10_colors[2])) +\n  labs(\n    title = \"bill depth vs bill width\",\n    subtitle = \"on Biscoe and Torgersen islands\",\n    x = \"bill depth (mm)\",\n    y = \"bill width (mm)\",\n    color = \"island\"\n  ) +\n  theme_classic(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n## check the %notin% operator\npenguins %&gt;%\n\n  filter(island %notin% c('Biscoe', 'Torgersen')) %&gt;%\n\n  select(species, island, bill_length_mm, bill_depth_mm, year) %&gt;%\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = island)) +\n  geom_point(size=3) +\n  scale_color_manual(values = c(\"Dream\" = tab10_colors[3])) +\n  labs(\n    title = \"bill depth vs bill width\",\n    subtitle = \"**not** on Biscoe and Torgersen islands\",\n    x = \"bill depth (mm)\",\n    y = \"bill width (mm)\",\n    color = \"island\"\n  ) +\n  theme_classic(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nAfter searching around, it seems like Hmisc::%nin%, and operators::%!in% libraries also replicate this functionality, but I still think it’s a cool example of reversing the behaviour of base functions using Negate()."
  },
  {
    "objectID": "posts/notin/index.html#negating-the-in-operator",
    "href": "posts/notin/index.html#negating-the-in-operator",
    "title": "%notin%",
    "section": "",
    "text": "I find the base %in% operator to be super useful to keep code readable in R, but sometimes I want everything but what I specify in my dplyr::filter call.\nThankfully, there’s a really nice way to do it by making use of the Negate() function. Here’s the code to run it.\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n})\n\n`%notin%` &lt;- Negate(`%in%`)\n\n\n\nLet’s try it out!\n\nsuppressPackageStartupMessages({\n  library(magrittr)\n\n  library(palmerpenguins)\n})\ndata(package = 'palmerpenguins')\n\n## get the head of the df\npenguins %&gt;%\n  head()\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n## get the first three colors from the Tableau10 palette\ntab10_colors &lt;- c(\"#4E79A7\", \"#F28E2B\", \"#E15759\")\n\n\n## print the unique islands\npenguins %$%\n  unique(island)\n\n[1] Torgersen Biscoe    Dream    \nLevels: Biscoe Dream Torgersen\n\n\n\n## check the %in% operator\npenguins %&gt;%\n\n  filter(island %in% c('Biscoe', 'Torgersen')) %&gt;%\n\n  select(species, island, bill_length_mm, bill_depth_mm, year) %&gt;%\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = island)) +\n  geom_point(size=3) +\n  scale_color_manual(values = c(\"Biscoe\" = tab10_colors[1], \"Torgersen\" = tab10_colors[2])) +\n  labs(\n    title = \"bill depth vs bill width\",\n    subtitle = \"on Biscoe and Torgersen islands\",\n    x = \"bill depth (mm)\",\n    y = \"bill width (mm)\",\n    color = \"island\"\n  ) +\n  theme_classic(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n## check the %notin% operator\npenguins %&gt;%\n\n  filter(island %notin% c('Biscoe', 'Torgersen')) %&gt;%\n\n  select(species, island, bill_length_mm, bill_depth_mm, year) %&gt;%\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = island)) +\n  geom_point(size=3) +\n  scale_color_manual(values = c(\"Dream\" = tab10_colors[3])) +\n  labs(\n    title = \"bill depth vs bill width\",\n    subtitle = \"**not** on Biscoe and Torgersen islands\",\n    x = \"bill depth (mm)\",\n    y = \"bill width (mm)\",\n    color = \"island\"\n  ) +\n  theme_classic(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nAfter searching around, it seems like Hmisc::%nin%, and operators::%!in% libraries also replicate this functionality, but I still think it’s a cool example of reversing the behaviour of base functions using Negate()."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "hierarchy",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hierarchy",
    "section": "",
    "text": "Welcome to hierarchy, a blog about using regression, causal inference, machine learning, and hierarchical (multilevel) modeling to distinguish signal from noise in data; usually by thinking about the data generating process.\nI graduated with my PhD from Columbia University in 2022, where the focus of my dissertation was the use of computational statistics to optimize experimental design of nonlinear, hierarchical models. I regularly use Python (TensorFlow, PyTorch, PyMC), R (brms, cmdstanr, tidyverse), and Stan (BridgeStan) to answer the questions I have about the information contained in my data. I’m passionate about quantifying uncertainty, stochastic optimization, forecasting, game theory, and scientific ML.\nFind me here, or send me an email :)"
  },
  {
    "objectID": "index.html#about-hierarchy",
    "href": "index.html#about-hierarchy",
    "title": "hierarchy",
    "section": "",
    "text": "Welcome to hierarchy, a blog about using regression, causal inference, machine learning, and hierarchical (multilevel) modeling to distinguish signal from noise in data; usually by thinking about the data generating process.\nI graduated with my PhD from Columbia University in 2022, where the focus of my dissertation was the use of computational statistics to optimize experimental design of nonlinear, hierarchical models. I regularly use Python (TensorFlow, PyTorch, PyMC), R (brms, cmdstanr, tidyverse), and Stan (BridgeStan) to answer the questions I have about the information contained in my data. I’m passionate about quantifying uncertainty, stochastic optimization, forecasting, game theory, and scientific ML.\nFind me here, or send me an email :)"
  },
  {
    "objectID": "posts/notin/index.html#citations",
    "href": "posts/notin/index.html#citations",
    "title": "%notin%",
    "section": "Citations",
    "text": "Citations\nThumbnail photo from Donna Patterson hosted on the NSF website."
  }
]