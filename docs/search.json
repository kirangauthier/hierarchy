[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "move this to the above,\nWelcome to hierarchy, a blog about using regression, causal inference, machine learning, and hierarchical (multilevel) modeling to distinguish signal from noise. More broadly, we’re going to use computational statistics to:\nI (Kiran) did my PhD in the Bishop Group at Columbia University, graduating in August 2022, where I defended my thesis on optimal experimental design for hierarchical, nonlinear systems to characterize and design autonomous behaviours in microrobots. You can find it here. My goal was to use external fields (think magnetic, acoustic) to encode behaviours into microscale colloids to help them autonomously sense their environment, and accomplish tasks in noisy environments, like suppressing dendrite formation in lithium ion batteries, swimming against the flow to deliver cargo, and sniffing your way through stochastic chemical gradients.\nThese are all part of the larger subset of distinguishing signal from noise, and the statistical models I worked on can be applied to autonomous driving, predicting venture captial risk, and selecting optimal dosing regimens for rare diseases, among many other applications. We’ll use tools from probability and information theory, and leverage modern compute and other tricks to answer these problems, refining our models and understanding as we go."
  },
  {
    "objectID": "about.html#some-initial-thoughts",
    "href": "about.html#some-initial-thoughts",
    "title": "About me",
    "section": "Some initial thoughts",
    "text": "Some initial thoughts\n\nPortage\nstealing from astronomy astronomy (not astrology, I checked), psychology, psychometrics, ecology, econometrics,\nadd the photo in here\n\n\nData quality is paramount\nI started thinking about the value of data early on in grad school, because my experiments took a long time to run and a lot of them gave me useless results. As an added complication, my setup varied slightly every day, so I had to constantly recalibrate my understanding of the underlying parameters of my system. That led me to thinking about experimental design to suggest maximally informative experiments conditional on the data that I’ve already observed. I highly recommend this excellent paper from Tom Loredo which got me started about thinking in terms of probabilities and distributions, and how we quantify information using entropy. See the section on experimental design for more info.\n\n\nWhy do I believe in statistical modeling?\nChris Rackauckas said in a talk that “the major advances in machine learning were due to encoding more structure into the model” with a subquote of “more structure = faster and better fits from data” and that sums it up pretty well. I think that building models that more closely resemble the “true” data generating process are our best hope of actually learning what’s going on under the hood. And although it’s fun to throw compute at a problem, I believe in failing fast and in the power of the iterative model building workflow often using coarse-grained models and approximate inference.\nThis perspective was perfectly summarized by George Box when he said that “all models are wrong, but some are useful”. If our models are only ever an approximation of reality, and our compute, and mental sanity are finite, then it’s worth it to see what we can get away with before we build out luxurious models.\nOur goal is then to find a model, \\(M\\), that is that is an efficient coarse-grained of reality,\n\n\n\nModels (\\(p(y \\mid \\theta, M)\\)) approximate the true relationship between design and data \\(q(y \\mid d)\\).\n\n\nThe framework that I developed is especially useful when you have a small amount of data, observed from noisy individuals in heterogeneous groups (hierarchy), and you need to find where to look next (design). Think “how much medicine should I treat a group of patients with rare disease given that they will all process the drug in distinctively different ways”? people are inherently different?\nBut I firmly believe that not all data is created equal, and focusing on acquiring high quality data that maximize the predictive power of our models will be the next revolution in science If you put bad data into a AI or ML algorithm it’ll fail without warning with potentially devastating consequences I’m sure you can empathize this with your work"
  },
  {
    "objectID": "about.html#any-qualms",
    "href": "about.html#any-qualms",
    "title": "About me",
    "section": "Any qualms",
    "text": "Any qualms\n,\nmultimodality\nfolk theorem of statistical computation\nintersection of what cna be done and what should be done, is the juice worth the squeeze\ndistinguish signal from noise\nhierarcchal models are everywhere\nExperimental design Other great papers to use as introductory material to experimental design are Huan and Marzouk’s paper, Dennis Lindley’s entire catalog, but this paper in particular, and Andrew Gelman’s entire catalog, footnotes, and this paper.\nWhy his footnotes? I can’t find the quote now (if you remember it, email me!) but it went something along the lines of “every original thought you’ve ever had about statistics has already been published by Andrew [Gelman] in a footnote of a paper from the 1970s”.\nI’m Kiran and I’m a computational statistician, graduating with my PhD from Columbia University other topics to figure out if the juice is worth the squeeze. I’ll be programming in Python, R, Stan, and Julia, and am a huge advocate of failing fast, model parsimony, and Occam’s razor, so we’ll be timing, multiprocessing, broadcasting, and using whichever backend can get us a close-enough answer fastest.\nI did my PhD in the Bishop group ………………………… I wrote my dissertation using PyMC3 (now PyMC), and have recently switched to Stan because I’ve been writing in R a lot more. This blog will also give me a chance to revisit PyMC, Pyro and NumPyro (built on top of PyTorch), TFP (built on top of TensorFlow), and Turing.jl to see which is fast and readable.\nI also think that visualization is a huge part of my workflow, which is why I’ve been super impressed with the ease-of-use of the brms package (not to mention Paul’s support on GitHub and the Stan Discourse), because getting inferential models to act generatively is pretty clunky and requires custom code to develop good visuals. brms takes care of this for you. To my knowledge, there’s no Python equivalent so we’ll flip back and forth depending on how far we are in the prototyping stage.\nMy favourite book right now is Richard McElreath’s “Statistical Rethinking”, and his accompanying lecture series which are both incredibly well written / produced, and are great discussions of challenges with asking the “oracle” of regression seemingly innocuous questions (see the height ~ +\n\n\n\nmodel for a great example).\nI believe that there’s huge value in modeling data hierarchially, where groups of individuals are drawn from a population and we’d like to have information flow between them, but these models are often slow, hard to fit, and are usually much harder to set up than conventional non-hierarchical models. I’m hoping to have a repository that acts as a bridge between the different languages, and packages, so that we can all benefit from the massive amount of documentation and knowledge that has already been shared.\nI may make errors, if you see one, please email kiran.gauthier@columbia.edu me or leave a comment and I’ll revise it!\nLibrary I’ll fill this later.\nOther blogs I read Gelman Betancourt Frank Harrell Aki Vehtari\nPosts to come BridgeStan Simpson’s paradox Bonferroni correction Talking about interactions\nWish list item: see how information flows between parameters, include something about synthetic data generation.\nAdd draft: true to the document options if you’d like a post to not be included in the listing, site map, or site search. For example: #— title: “My Post” description: “Post description” author: “Fizz McPhee” date: “5/22/2021” draft: true #—"
  },
  {
    "objectID": "posts/why-hierarchy/index.html",
    "href": "posts/why-hierarchy/index.html",
    "title": "Why hierarchical models?",
    "section": "",
    "text": "http://nakisa.org/bankr-useful-financial-r-snippets/hierarchical-linear-models/"
  },
  {
    "objectID": "posts/why-hierarchy/index.html#why-is-hierarchy-important",
    "href": "posts/why-hierarchy/index.html#why-is-hierarchy-important",
    "title": "Why hierarchical models?",
    "section": "Why is hierarchy important?",
    "text": "Why is hierarchy important?\nThings that look like other things can share information. This is the fundamental concept that underlies hierarchical modeling. … and it has the nice property of helping to stabilize our estimates, especially when there’s not a ton of data.\nIntuitively, this makes sense, if I know something about your neighbor, it might tell me something about you. If I treated you as independent entities, I’m probably throwing away information. There is a chance that you’re very dissimilar, but odds are you are more similar to people in your county / state / who share your same job / play the same position as you than you are for people in other groups."
  },
  {
    "objectID": "posts/why-hierarchy/index.html#modeling-home-runs",
    "href": "posts/why-hierarchy/index.html#modeling-home-runs",
    "title": "Why hierarchical models?",
    "section": "Modeling home runs",
    "text": "Modeling home runs\n\nPlayers as independent entities\nmodel sources of variation\ndifferentiate skill from random chance / variation\nfeatures of judging models\ndescriptivess - how well does your midel / metric correlate to an observation of interest in the same time period\npredictiveness - how well do they correlate in consequtive seasons / months\nreliability - how well does the metric correlate with itself in consequtive seasons\nREMOVE MLB, TRY KAGGLE, also, can start with intro to Bayesian stats from Kyle\nSay we’re trying to model home rates of home runs in the MLB, we could use a binomial distribution to estimate the probability of hitting a home run \\(\\theta\\) for each player in the league \\(1, \\dots, k\\), let’s assume it’s fixed for the given period we observe these players. Your likelihood for the \\(k^\\text{th}\\) player with \\(n\\) at bats and \\(h\\) home runs would look something like1:\n\\[\np(h \\mid n, \\theta) = {n \\choose h} \\theta^h (1 - \\theta)^{n-h}\n\\]\nor, graphically,\n![]\neasy way to do it is using gps which is basically a generalized multivariate normal distribution of infinite dimension can use statisitcal tricks with conditioning and margnixalizatio of multiN to use it in reality\nmean and covariance fucntion, cov func describes wigglyness and how jt changes over time can use Hilbert space GPs for big datasets to avoid the data matrix inversion - is an approximate GP\nIntuititvely, this should make sense, the odds of you hitting a home run at any given at bat are \\(\\theta\\). Hitting \\(h\\) home runs given \\(n\\) trials means you had \\(h\\) trials with probability \\(\\theta\\) and \\(n-h\\) trials where you didn’t, which must be realizations of the probability \\(1 - \\theta\\). So far we’re at \\(\\theta^h (1 - \\theta)^{n-h}\\). Taking care of some ordering with the \\({n}\\choose{h}\\), we account for the many ways that we could have had \\(h\\) home runs in \\(n\\) trials, and we end up with a normalized (this just means \\(\\int p(\\mathbf{x}) d\\mathbf{x} = 1\\)) probability density which has a mean value of \\(\\theta n\\) and a variance of \\(n \\theta (1 - \\theta)\\).\nThis should also make sense, given \\(n\\) trials, and an independent probability of success, \\(\\theta\\), we’d expect that the mean number of home runs is \\(\\mathcal{E}[h] = n \\theta\\). What’s interesting, however, is that our variance grows linearly with the number of at bats, as \\(\\text{var}(h) = n \\theta (1 - \\theta) \\propto n\\). So as we have more at bats, the more uncertain we are in the total number of home runs.\nadd the figures here… cite them too\ncomplication is low at bats\nThis is more formally known as partial pooling, summarized well here, and this\ndefaukt PyMC is C backend, can use pyTensor backend to speed things up by allowing different pieces to go to different backends, numpyro runs on JAX which enables GPU\nwhat’s the role of priors?\nsimpson’s paradox\nwhy are hierarchical models hard to fit?\npitchers look like other ptichers (not Shohei)\n\\[\np(h_k \\mid n_k, \\theta_k) = {n_k \\choose h_k} \\theta_k^{h_k} (1 - \\theta_k)^{n_k - h_k}\n\\]\nbut I like to drop indexes where I can to simplify notation."
  },
  {
    "objectID": "posts/why-hierarchy/index.html#comments-add-to-blog",
    "href": "posts/why-hierarchy/index.html#comments-add-to-blog",
    "title": "Why hierarchical models?",
    "section": "comments add to blog??",
    "text": "comments add to blog??"
  },
  {
    "objectID": "posts/why-hierarchy/index.html#footnotes",
    "href": "posts/why-hierarchy/index.html#footnotes",
    "title": "Why hierarchical models?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, we should have written the likelihood over the players as:↩︎"
  },
  {
    "objectID": "posts/notin/index.html",
    "href": "posts/notin/index.html",
    "title": "%notin%",
    "section": "",
    "text": "I find the base %in% operator to be super useful to keep code readable in R, but sometimes I want everything but what I specify in my dplyr::filter call.\nThankfully, there’s a really nice way to do it by making use of the Negate() function. Here’s the code to run it.\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n})\n\n`%notin%` &lt;- Negate(`%in%`)\n\n\n\nLet’s try it out!\n\nsuppressPackageStartupMessages({\n  library(magrittr)\n\n  library(palmerpenguins)\n})\ndata(package = 'palmerpenguins')\n\n## get the head of the df\npenguins %&gt;%\n  head()\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n## get the first three colors from the Tableau10 palette\ntab10_colors &lt;- c(\"#4E79A7\", \"#F28E2B\", \"#E15759\")\n\n\n## print the unique islands\npenguins %$%\n  unique(island)\n\n[1] Torgersen Biscoe    Dream    \nLevels: Biscoe Dream Torgersen\n\n\n\n## check the %in% operator\npenguins %&gt;%\n\n  filter(island %in% c('Biscoe', 'Torgersen')) %&gt;%\n\n  select(species, island, bill_length_mm, bill_depth_mm, year) %&gt;%\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = island)) +\n  geom_point(size=3) +\n  scale_color_manual(values = c(\"Biscoe\" = tab10_colors[1], \"Torgersen\" = tab10_colors[2])) +\n  labs(\n    title = \"bill depth vs bill width\",\n    subtitle = \"on Biscoe and Torgersen islands\",\n    x = \"bill depth (mm)\",\n    y = \"bill width (mm)\",\n    color = \"island\"\n  ) +\n  theme_classic(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n## check the %notin% operator\npenguins %&gt;%\n\n  filter(island %notin% c('Biscoe', 'Torgersen')) %&gt;%\n\n  select(species, island, bill_length_mm, bill_depth_mm, year) %&gt;%\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = island)) +\n  geom_point(size=3) +\n  scale_color_manual(values = c(\"Dream\" = tab10_colors[3])) +\n  labs(\n    title = \"bill depth vs bill width\",\n    subtitle = \"**not** on Biscoe and Torgersen islands\",\n    x = \"bill depth (mm)\",\n    y = \"bill width (mm)\",\n    color = \"island\"\n  ) +\n  theme_classic(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nAfter searching around, it seems like Hmisc::%nin%, and operators::%!in% libraries also replicate this functionality, but I still think it’s a cool example of reversing the behaviour of base functions using Negate()."
  },
  {
    "objectID": "posts/notin/index.html#negating-the-in-operator",
    "href": "posts/notin/index.html#negating-the-in-operator",
    "title": "%notin%",
    "section": "",
    "text": "I find the base %in% operator to be super useful to keep code readable in R, but sometimes I want everything but what I specify in my dplyr::filter call.\nThankfully, there’s a really nice way to do it by making use of the Negate() function. Here’s the code to run it.\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n})\n\n`%notin%` &lt;- Negate(`%in%`)\n\n\n\nLet’s try it out!\n\nsuppressPackageStartupMessages({\n  library(magrittr)\n\n  library(palmerpenguins)\n})\ndata(package = 'palmerpenguins')\n\n## get the head of the df\npenguins %&gt;%\n  head()\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n## get the first three colors from the Tableau10 palette\ntab10_colors &lt;- c(\"#4E79A7\", \"#F28E2B\", \"#E15759\")\n\n\n## print the unique islands\npenguins %$%\n  unique(island)\n\n[1] Torgersen Biscoe    Dream    \nLevels: Biscoe Dream Torgersen\n\n\n\n## check the %in% operator\npenguins %&gt;%\n\n  filter(island %in% c('Biscoe', 'Torgersen')) %&gt;%\n\n  select(species, island, bill_length_mm, bill_depth_mm, year) %&gt;%\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = island)) +\n  geom_point(size=3) +\n  scale_color_manual(values = c(\"Biscoe\" = tab10_colors[1], \"Torgersen\" = tab10_colors[2])) +\n  labs(\n    title = \"bill depth vs bill width\",\n    subtitle = \"on Biscoe and Torgersen islands\",\n    x = \"bill depth (mm)\",\n    y = \"bill width (mm)\",\n    color = \"island\"\n  ) +\n  theme_classic(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n## check the %notin% operator\npenguins %&gt;%\n\n  filter(island %notin% c('Biscoe', 'Torgersen')) %&gt;%\n\n  select(species, island, bill_length_mm, bill_depth_mm, year) %&gt;%\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = island)) +\n  geom_point(size=3) +\n  scale_color_manual(values = c(\"Dream\" = tab10_colors[3])) +\n  labs(\n    title = \"bill depth vs bill width\",\n    subtitle = \"**not** on Biscoe and Torgersen islands\",\n    x = \"bill depth (mm)\",\n    y = \"bill width (mm)\",\n    color = \"island\"\n  ) +\n  theme_classic(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nAfter searching around, it seems like Hmisc::%nin%, and operators::%!in% libraries also replicate this functionality, but I still think it’s a cool example of reversing the behaviour of base functions using Negate()."
  },
  {
    "objectID": "posts/notin/index.html#citations",
    "href": "posts/notin/index.html#citations",
    "title": "%notin%",
    "section": "Citations",
    "text": "Citations\nThumbnail photo from Donna Patterson hosted on the NSF website."
  },
  {
    "objectID": "posts/diffusion-bayes-part-1/index.html",
    "href": "posts/diffusion-bayes-part-1/index.html",
    "title": "diffusionBayes - part 1: simulated analysis of single trajectories",
    "section": "",
    "text": "Welcome to the first post in a multi-post series about analyzing diffusive particles trajectories to infer their uncertain diffusivities. All of the code for this project can be found by clicking the link in the titile of this post that’ll take you to the project repository.\nI’m a firm believer that good inference is enabled by becoming familiar with the data generating process as a way to (1) validate your ability to recover, “true”, hidden parameters, and (2) see how our models might fail to capture relevant features within the observed data.\nThis first post will consider simulated 2-dimensional random walkers with no directional preference (isotropic), which are perfectly well resolved by our camera, or observer, and are built into our simulate class. We’ll build in complexity as we go, including the opportunity for our particles to:\n\ndrift from convective flows\nget “stuck” on the experimental cell, or media (using potential wells)\nbe drawn from different populations, this one is really cool\nhave different sub- and super-diffusive behaviours\n\nwe’ll use a couple of different inference strategies, in our infer class:\n\nconventional mean squared displacement analysis\na Bayesian analog of the more conventional analysis\na population based model, again, really cool, especially with short, noisy tracks\n\ndiscriminating between “good”, and “bad” data using posterior predictive checks, in our check class:\n\nmean squared displacement\nradius of gyration\nradius of gyration for non-overlapping intervals\n\nwhile modulating some of the experimental parameters, in our experiment class:\n\nartificially reduce the signal-to-noise ratio\nintroduce uncertainty in the centroids of our diffusing particles\nbreak up our tracks (because of approximating quasi-3D environment as 2D, or particles fading into background noise)\nartificially constrain the length of our track a consequence of above, but this is the most important\n\nto see how that influences the inference of the diffusivity parameter, \\(\\widehat{D}\\), relative to the true diffusivity, \\(D_\\text{true}\\). When you have long tracks with little uncertainty in the diffusing particle’s position, any inference technique will work. But in the presence of short, noisy tracks, we’ll demonstrate that the posterior predictive checks and population based model significantly outperforms the conventional techniques in reliably estimating the diffusivities.\nFinally, we’ll turn it loose on some real, experimental data and see how it performs!\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport matplotlib as mpl\n\nfrom tqdm import tqdm\nfrom scipy import stats\n\nimport session_info\n\n%config InlineBackend.figure_format='retina'\n\nmpl.rc('font',family='Arial')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Arial']\nplt.rcParams['axes.linewidth'] = 0.5\nplt.rc('xtick', labelsize=7)      # fontsize of the tick labels\nplt.rc('ytick', labelsize=7)      # fontsize of the tick labels\n\nplt.rcParams['figure.figsize'] = (4, 3)\nplt.rcParams['font.size'] = 6\n\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nIn order to make the code as reproducible as possible, I wanted to initialize some classes so that once everything is built out, you can initialize the class, call the method that you want to simulate, infer, check, or plot, change the parameters of the experiment around, and see how that impacts our results.\nWe’re going to start with simulate, and write some of the simplest infer and plot functions for an idealized experiment.\n\nclass Simulate:\n    \"\"\"\n    A class for simulating diffusion processes.\n\n    Methods:\n        generate_brownianMotion: Generates a single isotropic 2D particle trajectory.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n\n\nLet’s set out some notation to take forward, the 2D trajectory of the \\(j\\)-th particle in the \\(i\\)-th experiment will be summarized by \\(\\mathbf{X}_{ij}\\), where the coordinates of the \\(k\\)-th time point are \\(\\[x_{ijk}, y_{ijk}\\]\\) respectively. The entire trajectory is then given by\n\\[\nX_{ij} = \\begin{bmatrix}\nx_{ij1} & y_{ij1} \\\\\nx_{ij2} & y_{ij2} \\\\\n\\vdots & \\vdots \\\\\nx_{ijk} & y_{ijk}\n\\end{bmatrix}\n\\]\nThe reason why it’s useful to do it this way is that our diffusive particles are often captured for varying lengths of time, or numbers of frames, so \\(n_k\\), the total number of frames observed for the \\(k\\)-th particle varies and prevents us from using a nice numpy.array() to contain all of our trajectories.\nInstead, we’ll use a ragged array, or list of lists structure. All of our functions will accept a single 2D trajectory \\(\\mathbf{X}_{ij}\\) which contains all of the tracked data for that particle alone. Luckily, a lot of the models that we’re working with are conjugate (more on this below) or easy enough to work with that it doesn’t pose a big issue. We can always decorate our code with an @jit from numba or leverage JAX if we need to speed things up.\n\n\n\nWe’ll compute everything under the hood in units of pixels and frames, for example, the units of diffusivity would be \\(\\text{px}^2 / \\text{frame}\\) under the hood, but will be converted to say \\(\\mu \\text{m}^2 / s\\) by setting an argument like return_real_units=True.\nIn terms of reproducibility, it’s also super important to consider the seeding of each of these functions. You’ll see an argument called base_seed which ensures that whenever we’re sampling random numbers, we can preserve them from run to run.\n\n\n\nLet’s initialize our classes, starting with simulate. All of the functions in these posts will be contained in files that are named by the class, so in this case it’ll be simulate.py. You can find the file here.\nIn the simplest case, our particles will perform 2D random walks, and I’m going to get a bit loose with the notation we just set forth to help things not get too cluttered too early. Wherever I say \\(\\mathbf{X}_k\\) I really mean \\(\\mathbf{X}_{ijk}\\) so the \\(k\\)-th time point of the \\(j\\)-th particle in experiment \\(i\\) is just the \\(k\\)-th time point. This doesn’t butcher our interpretation too badly because our particle’s motion is assumed to be uncorrelated with eachother, and you’ll see why removing the \\(i\\) and \\(j\\) subscripts helps below.\nWe can express the likelihood of sequential steps \\(\\mathbf{X}_{k-1} = [x_{k-1}, y_{k-1}]\\) and \\(\\mathbf{X}_k\\), during a given time interval \\(\\tau_k = t_k - t_{k-1}\\) as:\n\\[\np(\\mathbf{X}_k \\mid \\mathbf{X}_{k-1}, D, \\tau_k) = \\frac{1}{\\sqrt{4 \\pi D \\tau_k}} \\exp{\\left(- \\frac{r_k^2}{4 D \\tau_k} \\right)}\n\\]\nwhere \\(r_k^2 = (x_k - x_{k-1})^2 + (y_k - y_{k-1})^2\\) is the 2D displacement of the particle during the time interval \\(\\tau_k\\). If you feel comfortable with the above equation, feel free to skip to the next paragraph, otherwise, read the dropdown note and I’ll tell you how I would make sense of the above because I want everyone to stay on board and engaged for as long as they can.\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote: When we express probabilities, such as \\(p(\\mathbf{X}_k \\mid \\mathbf{X}_{k-1}, D, \\tau_k)\\), variables on the right side of the line (in this case \\(\\mathbf{X}_{k-1}, D, \\tau_k\\)) are assumed to be known. Some people or textbooks might say that we’re conditioning on these variables, all that means is that we’re forming our opinion of our unknowns given the things on the right side of the line. So what do we not know? Whatever’s on the left side of the line, in this case it’s \\(\\mathbf{X}_k\\). Some people might call the variables on the left side of the line random, or latent, variables. All that means is that we don’t know their values with certainty. So I would read this as “conditional on our previous position \\(\\mathbf{X}_{k-1}\\), the diffusivity of our particle \\(D\\), and time lag \\(\\tau_k\\), the likelihoood of the current position of our particle \\(\\mathbf{X}_k\\) is given by the probability density \\(\\frac{1}{\\sqrt{4 \\pi D \\tau_k}} \\exp{\\left(- \\frac{r_k^2}{4 D \\tau_k} \\right)}\\)”.\n\n\n\nCool, let’s keep going. Now that we’ve considered a single jump between two time points, the rest of the trajectory is pretty simple because in this model, all displacements are assumed to be uncorrelated.\n\\[\np(\\mathbf{X}_k \\mid D, \\tau_k) = p(\\mathbf{X}_0) \\prod_{k=1}^K \\frac{1}{4\\pi D \\tau_k} \\exp \\left( -\\frac{r_k^2}{4D \\tau_k} \\right)\n\\]\nwhere the only new thing I’ve added is the probability of the initial starting point of the particle, \\(\\mathbf{X}_0\\), which is assumed to be uniform across the viewing area.\n\n\nCool, so each displacement in 2D is assumed to have variance \\(4 D \\tau_k\\), which is the sum of independent displacements of \\(2 D \\tau_k\\) in each direction, let’s implement this below and drop it into our simulate.py file. Expand the below notes on alternatives to this implementation\n\ndef generate_brownianMotion(D, n_steps, X_start, tau, base_seed):\n    \"\"\"\n    Generate a single isotropic 2D particle trajectory.\n\n    Parameters:\n    - D (float): Diffusion coefficient [px^2 / frame]\n    - n_steps (int): Number of steps in the trajectory [frame]\n    - X_start (tuple): Position at time 0 (x, y) [px, px]\n    - tau (float): Time step [frame]\n    - base_seed (int): Random seed for this trajectory\n\n    Returns:\n    - x, y: Arrays containing the x and y positions of the trajectory [px]\n\n    Note:\n    This function uses a independent Gaussian steps to simulate Brownian motion.\n    The x and y dimensions are assumed to be independent.\n    \"\"\"\n\n    np.random.seed(base_seed)\n\n    x = np.zeros(n_steps)\n    y = np.zeros(n_steps)\n\n    x[0], y[0] = X_start\n\n    for k in range(1, n_steps):\n      std_dev = np.sqrt(2 * D * tau)\n      x[k] = x[k-1] + np.random.normal(0, std_dev)\n      y[k] = y[k-1] + np.random.normal(0, std_dev)\n\n    return x, y\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nGetting fancy: We could also have written the generate_brownianMotion() function as if it were generated from a multivariate Gaussian of variance \\(4 D \\tau_k\\), the results will be the same:\ndef generate_brownianMotion(D, n_steps, X_start, tau, base_seed):\n  # ...\n  np.random.seed(base_seed)\n  x = np.zeros(n_steps)\n  y = np.zeros(n_steps)\n\n  x[0], y[0] = X_start\n\n  # Define the covariance matrix for the multivariate Gaussian\n  covariance_matrix = [[4*D*tau, 0], [0, 4*D*tau]]  # Independent x and y with variance 4Dtau respectively\n\n  for k in range(1, n_steps):\n      step = np.random.multivariate_normal([0, 0], covariance_matrix)\n      x[k] = x[k-1] + step[0]\n      y[k] = y[k-1] + step[1]\n\n  return x, y\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor the sake of efficiency however, you’ll see that the final implementation takes advantage of the independence of all jumps and instead computes all of the displacements at once, taking the cumulative sum of their displacements.\n  x[0], y[0] = X_start\n\n  x_traj = np.random.normal(loc=0, scale=2*D*tau, size=t.shape[0]-1)\n  y_traj = np.random.normal(loc=0, scale=2*D*tau, size=t.shape[0]-1)\n\n  x[1:] = x[0] + np.cumsum(x_traj)\n  y[1:] = y[0] + np.cumsum(y_traj)\n\n  return x, y\n\n\n\nLet’s visualize the trajectory!\n\nx, y = generate_brownianMotion(D=0.1, n_steps=300, X_start=(0, 0), tau=1, base_seed=4444)\n\nplt.figure(figsize=(4, 3))\nplt.plot(x, y, color=firebrick4)\nplt.axis('equal')\nplt.xticks([0, -5, -10, -15])\nplt.yticks([0, -5, -10, -15])\nplt.xlabel('x [px]')\nplt.ylabel('y [px]')\npass\n\n\n\n\n\n\n\n\nLooks great.\n\n\n\n\nIn this case, we know exactly what the true diffusion coefficient, \\(D_\\text{true}\\) is because we set it to 1 \\(\\text{px}^2 / \\text{frame}\\). Let’s see if we can recover it, and also add our first couple of functions to our infer and plot classes.\n\n\n\nAs we said, we have single particle trajectories in our list of lists, and we want to highlight how short tracks of diffusing particles can be inferred reliably using a hierarchical Bayesian model. At the moment though, we have a really long, perfectly resolved track, so the conventional analysis should be just fine. Remember we have our single particle trajectory, \\(\\mathbf{X}_k = [x_k, y_k]\\) where again I’ve dropped the \\(i\\) and \\(j\\) indices to help with clutter. The mean squared displacement (MSD) of a particle during a time lag \\(\\tau\\), \\(\\text{msd}(\\tau)\\) evolves linearly in the case of Brownian diffusion, \\(\\text{msd}(\\tau) = 4 D \\tau\\) (in \\(2\\text{D}\\)). For arbitrary \\(\\tau\\), the MSD is calculated by:\n\\[\n\\text{msd}(\\tau) = \\frac{1}{N-\\tau} \\sum_{k=1}^{N-\\tau} [(x_{k + \\tau} - x_k)^2 + (y_{k + \\tau} - y_k)^2]\n\\]\nSo all it takes is for us to plot the \\(\\text{msd}\\) for various values of \\(\\tau\\) to a line to estimate the value \\(\\widehat{D}\\) (if you’re curious why my \\(D\\) is wearing a hat, click below).\n:::{callout-tip collapse=“true”} Because we usually don’t know the true value of a parameter, you’ll see me write it’s estimate with a hat, as in \\(\\widehat{D}\\) to tell you that it’s being fit from the data. This is in constrast to the true value that we had set up earlier in the simualtion, \\(D_\\text{true}\\), which has no hat. :::\nAs \\(\\tau\\) gets larger and larger, we’ll have less and less data to compute the mean displacement of our particle. To be safe in our averaging, we’ll only use lags from the first 1/3 of the trajectory.\n\ndef calculate_msd(trajectory):\n  x, y = trajectory\n  t = np.arange(len(x))\n  lags = np.unique(t // 3).astype(int)[::2]\n\n  msd = np.zeros((lags.shape[0], ))\n  for i, lag in enumerate(lags):\n    dxlag = np.diff(x[::lag])\n    dylag = np.diff(y[::lag])\n    J = dxlag.shape[0]\n\n    if J == 1:\n      continue\n    else:\n      msd[i] = np.sum( (dxlag - dxlag.mean())**2 + (dylag - dylag.mean())**2 ) / J\n\n  return msd\n\n# def fit_msd(trajectory):\n\n\n\n\n\ndef infer_diffusivity(trajectory, inference_step=1, drift=True):\n  x, y = trajectory\n\n  ## compute displacements\n  idx = (np.mod(t, inference_step)==0)\n  dt = t[idx][1:] - t[idx][0:-1]\n  dx = x[idx][1:] - x[idx][0:-1]\n  dy = y[idx][1:] - y[idx][0:-1]\n\n  K = dx.shape[0]\n\n  ## estimate drift parameters\n  if drift:\n    Uhat = np.sum(dx) / np.sum(dt)\n    Vhat = np.sum(dy) / np.sum(dt)\n\n    alpha = K - 2\n    beta = np.sum(( (dx - Uhat*dt)**2 + (dy - Vhat*dt)**2 ) / (4*dt))\n\n  ## compute posterior parameters for inverse gamma distribution\n  else:\n    alpha = K - 1\n    beta = np.sum( (dx**2 + dy**2) / (4*dt) )\n\n  return alpha, beta\n\ndef invGamma_toDiffusivity(alpha, beta, mode=True, point_estimate=False, interval=0.05):\n  ## return the mode\n  if mode and point_estimate:\n    mode = beta / (alpha + 1)\n    return mode\n\n  ## return the mean\n  if mode==False and point_estimate:\n    mean = beta / (alpha - 1) ## give an error if alpha not &gt; 1\n    return mean\n\n  if mode and point_estimate==False:\n    mode = beta / (alpha + 1)\n\n    lower = scipy.stats.invgamma.ppf(interval / 2, a=alpha, scale=beta)\n    upper = scipy.stats.invgamma.ppf((1-interval)/2, a=alpha, scale=beta)\n\n    return mode, lower, upper\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'}\n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n\n\n\nThumbnail photo from Donna Patterson hosted on the NSF website.\n\n\n\n\nsession_info.show()\n\n\nClick to view session information\n-----\nmatplotlib          3.8.4\nnumpy               1.26.4\nscipy               1.13.0\nsession_info        1.0.0\nsimulate            NA\ntqdm                4.66.4\n-----\n\n\nClick to view modules imported as dependencies\nPIL                         10.3.0\nanyio                       NA\nappnope                     0.1.3\nasttokens                   NA\nattr                        23.1.0\nattrs                       23.1.0\nbabel                       2.11.0\nbrotli                      1.0.9\ncertifi                     2024.02.02\ncffi                        1.16.0\ncharset_normalizer          2.0.4\ncolorama                    0.4.6\ncomm                        0.2.1\ncycler                      0.10.0\ncython_runtime              NA\ndateutil                    2.8.2\ndebugpy                     1.6.7\ndecorator                   5.1.1\ndefusedxml                  0.7.1\nexecuting                   0.8.3\nfastjsonschema              NA\nidna                        3.7\nipykernel                   6.28.0\njedi                        0.18.1\njinja2                      3.1.3\njson5                       NA\njsonschema                  4.19.2\njsonschema_specifications   NA\njupyter_events              0.8.0\njupyter_server              2.10.0\njupyterlab_server           2.25.1\nkiwisolver                  1.4.4\nmarkupsafe                  2.1.3\nmatplotlib_inline           0.1.6\nmpl_toolkits                NA\nnbformat                    5.9.2\noverrides                   NA\npackaging                   23.2\npandas                      2.2.2\nparso                       0.8.3\npexpect                     4.8.0\npkg_resources               NA\nplatformdirs                3.10.0\nprometheus_client           NA\nprompt_toolkit              3.0.43\npsutil                      5.9.0\nptyprocess                  0.7.0\npure_eval                   0.2.2\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.9.5\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.15.1\npyparsing                   3.0.9\npythonjsonlogger            NA\npytz                        2024.1\nreferencing                 NA\nrequests                    2.31.0\nrfc3339_validator           0.1.4\nrfc3986_validator           0.1.1\nrpds                        NA\nsend2trash                  NA\nsix                         1.16.0\nsniffio                     1.3.0\nsocks                       1.7.1\nstack_data                  0.2.0\ntornado                     6.3.3\ntraitlets                   5.7.1\nunicodedata2                NA\nurllib3                     2.2.1\nwcwidth                     0.2.5\nwebsocket                   1.8.0\nyaml                        6.0.1\nzmq                         25.1.2\n\n \n-----\nIPython             8.20.0\njupyter_client      8.6.0\njupyter_core        5.5.0\njupyterlab          4.0.11\nnotebook            7.0.8\n-----\nPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:54:21) [Clang 16.0.6 ]\nmacOS-13.6.1-arm64-arm-64bit\n-----\nSession information updated at 2024-08-02 15:24"
  },
  {
    "objectID": "posts/diffusion-bayes-part-1/index.html#part-1---simulating-and-analyzing-individual-trajectories",
    "href": "posts/diffusion-bayes-part-1/index.html#part-1---simulating-and-analyzing-individual-trajectories",
    "title": "diffusionBayes - part 1: simulated analysis of single trajectories",
    "section": "",
    "text": "Welcome to the first post in a multi-post series about analyzing diffusive particles trajectories to infer their uncertain diffusivities. All of the code for this project can be found by clicking the link in the titile of this post that’ll take you to the project repository.\nI’m a firm believer that good inference is enabled by becoming familiar with the data generating process as a way to (1) validate your ability to recover, “true”, hidden parameters, and (2) see how our models might fail to capture relevant features within the observed data.\nThis first post will consider simulated 2-dimensional random walkers with no directional preference (isotropic), which are perfectly well resolved by our camera, or observer, and are built into our simulate class. We’ll build in complexity as we go, including the opportunity for our particles to:\n\ndrift from convective flows\nget “stuck” on the experimental cell, or media (using potential wells)\nbe drawn from different populations, this one is really cool\nhave different sub- and super-diffusive behaviours\n\nwe’ll use a couple of different inference strategies, in our infer class:\n\nconventional mean squared displacement analysis\na Bayesian analog of the more conventional analysis\na population based model, again, really cool, especially with short, noisy tracks\n\ndiscriminating between “good”, and “bad” data using posterior predictive checks, in our check class:\n\nmean squared displacement\nradius of gyration\nradius of gyration for non-overlapping intervals\n\nwhile modulating some of the experimental parameters, in our experiment class:\n\nartificially reduce the signal-to-noise ratio\nintroduce uncertainty in the centroids of our diffusing particles\nbreak up our tracks (because of approximating quasi-3D environment as 2D, or particles fading into background noise)\nartificially constrain the length of our track a consequence of above, but this is the most important\n\nto see how that influences the inference of the diffusivity parameter, \\(\\widehat{D}\\), relative to the true diffusivity, \\(D_\\text{true}\\). When you have long tracks with little uncertainty in the diffusing particle’s position, any inference technique will work. But in the presence of short, noisy tracks, we’ll demonstrate that the posterior predictive checks and population based model significantly outperforms the conventional techniques in reliably estimating the diffusivities.\nFinally, we’ll turn it loose on some real, experimental data and see how it performs!"
  },
  {
    "objectID": "posts/diffusion-bayes-part-1/index.html#load-libraries",
    "href": "posts/diffusion-bayes-part-1/index.html#load-libraries",
    "title": "diffusionBayes - part 1: simulated analysis of single trajectories",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport matplotlib as mpl\n\nfrom tqdm import tqdm\nfrom scipy import stats\n\nimport session_info\n\n%config InlineBackend.figure_format='retina'\n\nmpl.rc('font',family='Arial')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Arial']\nplt.rcParams['axes.linewidth'] = 0.5\nplt.rc('xtick', labelsize=7)      # fontsize of the tick labels\nplt.rc('ytick', labelsize=7)      # fontsize of the tick labels\n\nplt.rcParams['figure.figsize'] = (4, 3)\nplt.rcParams['font.size'] = 6\n\n%load_ext autoreload\n%autoreload 2"
  },
  {
    "objectID": "posts/diffusion-bayes-part-1/index.html#classes-and-notation",
    "href": "posts/diffusion-bayes-part-1/index.html#classes-and-notation",
    "title": "diffusionBayes - part 1: simulated analysis of single trajectories",
    "section": "",
    "text": "In order to make the code as reproducible as possible, I wanted to initialize some classes so that once everything is built out, you can initialize the class, call the method that you want to simulate, infer, check, or plot, change the parameters of the experiment around, and see how that impacts our results.\nWe’re going to start with simulate, and write some of the simplest infer and plot functions for an idealized experiment.\n\nclass Simulate:\n    \"\"\"\n    A class for simulating diffusion processes.\n\n    Methods:\n        generate_brownianMotion: Generates a single isotropic 2D particle trajectory.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n\n\nLet’s set out some notation to take forward, the 2D trajectory of the \\(j\\)-th particle in the \\(i\\)-th experiment will be summarized by \\(\\mathbf{X}_{ij}\\), where the coordinates of the \\(k\\)-th time point are \\(\\[x_{ijk}, y_{ijk}\\]\\) respectively. The entire trajectory is then given by\n\\[\nX_{ij} = \\begin{bmatrix}\nx_{ij1} & y_{ij1} \\\\\nx_{ij2} & y_{ij2} \\\\\n\\vdots & \\vdots \\\\\nx_{ijk} & y_{ijk}\n\\end{bmatrix}\n\\]\nThe reason why it’s useful to do it this way is that our diffusive particles are often captured for varying lengths of time, or numbers of frames, so \\(n_k\\), the total number of frames observed for the \\(k\\)-th particle varies and prevents us from using a nice numpy.array() to contain all of our trajectories.\nInstead, we’ll use a ragged array, or list of lists structure. All of our functions will accept a single 2D trajectory \\(\\mathbf{X}_{ij}\\) which contains all of the tracked data for that particle alone. Luckily, a lot of the models that we’re working with are conjugate (more on this below) or easy enough to work with that it doesn’t pose a big issue. We can always decorate our code with an @jit from numba or leverage JAX if we need to speed things up.\n\n\n\nWe’ll compute everything under the hood in units of pixels and frames, for example, the units of diffusivity would be \\(\\text{px}^2 / \\text{frame}\\) under the hood, but will be converted to say \\(\\mu \\text{m}^2 / s\\) by setting an argument like return_real_units=True.\nIn terms of reproducibility, it’s also super important to consider the seeding of each of these functions. You’ll see an argument called base_seed which ensures that whenever we’re sampling random numbers, we can preserve them from run to run.\n\n\n\nLet’s initialize our classes, starting with simulate. All of the functions in these posts will be contained in files that are named by the class, so in this case it’ll be simulate.py. You can find the file here.\nIn the simplest case, our particles will perform 2D random walks, and I’m going to get a bit loose with the notation we just set forth to help things not get too cluttered too early. Wherever I say \\(\\mathbf{X}_k\\) I really mean \\(\\mathbf{X}_{ijk}\\) so the \\(k\\)-th time point of the \\(j\\)-th particle in experiment \\(i\\) is just the \\(k\\)-th time point. This doesn’t butcher our interpretation too badly because our particle’s motion is assumed to be uncorrelated with eachother, and you’ll see why removing the \\(i\\) and \\(j\\) subscripts helps below.\nWe can express the likelihood of sequential steps \\(\\mathbf{X}_{k-1} = [x_{k-1}, y_{k-1}]\\) and \\(\\mathbf{X}_k\\), during a given time interval \\(\\tau_k = t_k - t_{k-1}\\) as:\n\\[\np(\\mathbf{X}_k \\mid \\mathbf{X}_{k-1}, D, \\tau_k) = \\frac{1}{\\sqrt{4 \\pi D \\tau_k}} \\exp{\\left(- \\frac{r_k^2}{4 D \\tau_k} \\right)}\n\\]\nwhere \\(r_k^2 = (x_k - x_{k-1})^2 + (y_k - y_{k-1})^2\\) is the 2D displacement of the particle during the time interval \\(\\tau_k\\). If you feel comfortable with the above equation, feel free to skip to the next paragraph, otherwise, read the dropdown note and I’ll tell you how I would make sense of the above because I want everyone to stay on board and engaged for as long as they can.\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote: When we express probabilities, such as \\(p(\\mathbf{X}_k \\mid \\mathbf{X}_{k-1}, D, \\tau_k)\\), variables on the right side of the line (in this case \\(\\mathbf{X}_{k-1}, D, \\tau_k\\)) are assumed to be known. Some people or textbooks might say that we’re conditioning on these variables, all that means is that we’re forming our opinion of our unknowns given the things on the right side of the line. So what do we not know? Whatever’s on the left side of the line, in this case it’s \\(\\mathbf{X}_k\\). Some people might call the variables on the left side of the line random, or latent, variables. All that means is that we don’t know their values with certainty. So I would read this as “conditional on our previous position \\(\\mathbf{X}_{k-1}\\), the diffusivity of our particle \\(D\\), and time lag \\(\\tau_k\\), the likelihoood of the current position of our particle \\(\\mathbf{X}_k\\) is given by the probability density \\(\\frac{1}{\\sqrt{4 \\pi D \\tau_k}} \\exp{\\left(- \\frac{r_k^2}{4 D \\tau_k} \\right)}\\)”.\n\n\n\nCool, let’s keep going. Now that we’ve considered a single jump between two time points, the rest of the trajectory is pretty simple because in this model, all displacements are assumed to be uncorrelated.\n\\[\np(\\mathbf{X}_k \\mid D, \\tau_k) = p(\\mathbf{X}_0) \\prod_{k=1}^K \\frac{1}{4\\pi D \\tau_k} \\exp \\left( -\\frac{r_k^2}{4D \\tau_k} \\right)\n\\]\nwhere the only new thing I’ve added is the probability of the initial starting point of the particle, \\(\\mathbf{X}_0\\), which is assumed to be uniform across the viewing area.\n\n\nCool, so each displacement in 2D is assumed to have variance \\(4 D \\tau_k\\), which is the sum of independent displacements of \\(2 D \\tau_k\\) in each direction, let’s implement this below and drop it into our simulate.py file. Expand the below notes on alternatives to this implementation\n\ndef generate_brownianMotion(D, n_steps, X_start, tau, base_seed):\n    \"\"\"\n    Generate a single isotropic 2D particle trajectory.\n\n    Parameters:\n    - D (float): Diffusion coefficient [px^2 / frame]\n    - n_steps (int): Number of steps in the trajectory [frame]\n    - X_start (tuple): Position at time 0 (x, y) [px, px]\n    - tau (float): Time step [frame]\n    - base_seed (int): Random seed for this trajectory\n\n    Returns:\n    - x, y: Arrays containing the x and y positions of the trajectory [px]\n\n    Note:\n    This function uses a independent Gaussian steps to simulate Brownian motion.\n    The x and y dimensions are assumed to be independent.\n    \"\"\"\n\n    np.random.seed(base_seed)\n\n    x = np.zeros(n_steps)\n    y = np.zeros(n_steps)\n\n    x[0], y[0] = X_start\n\n    for k in range(1, n_steps):\n      std_dev = np.sqrt(2 * D * tau)\n      x[k] = x[k-1] + np.random.normal(0, std_dev)\n      y[k] = y[k-1] + np.random.normal(0, std_dev)\n\n    return x, y\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nGetting fancy: We could also have written the generate_brownianMotion() function as if it were generated from a multivariate Gaussian of variance \\(4 D \\tau_k\\), the results will be the same:\ndef generate_brownianMotion(D, n_steps, X_start, tau, base_seed):\n  # ...\n  np.random.seed(base_seed)\n  x = np.zeros(n_steps)\n  y = np.zeros(n_steps)\n\n  x[0], y[0] = X_start\n\n  # Define the covariance matrix for the multivariate Gaussian\n  covariance_matrix = [[4*D*tau, 0], [0, 4*D*tau]]  # Independent x and y with variance 4Dtau respectively\n\n  for k in range(1, n_steps):\n      step = np.random.multivariate_normal([0, 0], covariance_matrix)\n      x[k] = x[k-1] + step[0]\n      y[k] = y[k-1] + step[1]\n\n  return x, y\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFor the sake of efficiency however, you’ll see that the final implementation takes advantage of the independence of all jumps and instead computes all of the displacements at once, taking the cumulative sum of their displacements.\n  x[0], y[0] = X_start\n\n  x_traj = np.random.normal(loc=0, scale=2*D*tau, size=t.shape[0]-1)\n  y_traj = np.random.normal(loc=0, scale=2*D*tau, size=t.shape[0]-1)\n\n  x[1:] = x[0] + np.cumsum(x_traj)\n  y[1:] = y[0] + np.cumsum(y_traj)\n\n  return x, y\n\n\n\nLet’s visualize the trajectory!\n\nx, y = generate_brownianMotion(D=0.1, n_steps=300, X_start=(0, 0), tau=1, base_seed=4444)\n\nplt.figure(figsize=(4, 3))\nplt.plot(x, y, color=firebrick4)\nplt.axis('equal')\nplt.xticks([0, -5, -10, -15])\nplt.yticks([0, -5, -10, -15])\nplt.xlabel('x [px]')\nplt.ylabel('y [px]')\npass\n\n\n\n\n\n\n\n\nLooks great.\n\n\n\n\nIn this case, we know exactly what the true diffusion coefficient, \\(D_\\text{true}\\) is because we set it to 1 \\(\\text{px}^2 / \\text{frame}\\). Let’s see if we can recover it, and also add our first couple of functions to our infer and plot classes.\n\n\n\nAs we said, we have single particle trajectories in our list of lists, and we want to highlight how short tracks of diffusing particles can be inferred reliably using a hierarchical Bayesian model. At the moment though, we have a really long, perfectly resolved track, so the conventional analysis should be just fine. Remember we have our single particle trajectory, \\(\\mathbf{X}_k = [x_k, y_k]\\) where again I’ve dropped the \\(i\\) and \\(j\\) indices to help with clutter. The mean squared displacement (MSD) of a particle during a time lag \\(\\tau\\), \\(\\text{msd}(\\tau)\\) evolves linearly in the case of Brownian diffusion, \\(\\text{msd}(\\tau) = 4 D \\tau\\) (in \\(2\\text{D}\\)). For arbitrary \\(\\tau\\), the MSD is calculated by:\n\\[\n\\text{msd}(\\tau) = \\frac{1}{N-\\tau} \\sum_{k=1}^{N-\\tau} [(x_{k + \\tau} - x_k)^2 + (y_{k + \\tau} - y_k)^2]\n\\]\nSo all it takes is for us to plot the \\(\\text{msd}\\) for various values of \\(\\tau\\) to a line to estimate the value \\(\\widehat{D}\\) (if you’re curious why my \\(D\\) is wearing a hat, click below).\n:::{callout-tip collapse=“true”} Because we usually don’t know the true value of a parameter, you’ll see me write it’s estimate with a hat, as in \\(\\widehat{D}\\) to tell you that it’s being fit from the data. This is in constrast to the true value that we had set up earlier in the simualtion, \\(D_\\text{true}\\), which has no hat. :::\nAs \\(\\tau\\) gets larger and larger, we’ll have less and less data to compute the mean displacement of our particle. To be safe in our averaging, we’ll only use lags from the first 1/3 of the trajectory.\n\ndef calculate_msd(trajectory):\n  x, y = trajectory\n  t = np.arange(len(x))\n  lags = np.unique(t // 3).astype(int)[::2]\n\n  msd = np.zeros((lags.shape[0], ))\n  for i, lag in enumerate(lags):\n    dxlag = np.diff(x[::lag])\n    dylag = np.diff(y[::lag])\n    J = dxlag.shape[0]\n\n    if J == 1:\n      continue\n    else:\n      msd[i] = np.sum( (dxlag - dxlag.mean())**2 + (dylag - dylag.mean())**2 ) / J\n\n  return msd\n\n# def fit_msd(trajectory):\n\n\n\n\n\ndef infer_diffusivity(trajectory, inference_step=1, drift=True):\n  x, y = trajectory\n\n  ## compute displacements\n  idx = (np.mod(t, inference_step)==0)\n  dt = t[idx][1:] - t[idx][0:-1]\n  dx = x[idx][1:] - x[idx][0:-1]\n  dy = y[idx][1:] - y[idx][0:-1]\n\n  K = dx.shape[0]\n\n  ## estimate drift parameters\n  if drift:\n    Uhat = np.sum(dx) / np.sum(dt)\n    Vhat = np.sum(dy) / np.sum(dt)\n\n    alpha = K - 2\n    beta = np.sum(( (dx - Uhat*dt)**2 + (dy - Vhat*dt)**2 ) / (4*dt))\n\n  ## compute posterior parameters for inverse gamma distribution\n  else:\n    alpha = K - 1\n    beta = np.sum( (dx**2 + dy**2) / (4*dt) )\n\n  return alpha, beta\n\ndef invGamma_toDiffusivity(alpha, beta, mode=True, point_estimate=False, interval=0.05):\n  ## return the mode\n  if mode and point_estimate:\n    mode = beta / (alpha + 1)\n    return mode\n\n  ## return the mean\n  if mode==False and point_estimate:\n    mean = beta / (alpha - 1) ## give an error if alpha not &gt; 1\n    return mean\n\n  if mode and point_estimate==False:\n    mode = beta / (alpha + 1)\n\n    lower = scipy.stats.invgamma.ppf(interval / 2, a=alpha, scale=beta)\n    upper = scipy.stats.invgamma.ppf((1-interval)/2, a=alpha, scale=beta)\n\n    return mode, lower, upper\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'}\n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/diffusion-bayes-part-1/index.html#citations",
    "href": "posts/diffusion-bayes-part-1/index.html#citations",
    "title": "diffusionBayes - part 1: simulated analysis of single trajectories",
    "section": "",
    "text": "Thumbnail photo from Donna Patterson hosted on the NSF website."
  },
  {
    "objectID": "posts/diffusion-bayes-part-1/index.html#session-info",
    "href": "posts/diffusion-bayes-part-1/index.html#session-info",
    "title": "diffusionBayes - part 1: simulated analysis of single trajectories",
    "section": "",
    "text": "session_info.show()\n\n\nClick to view session information\n-----\nmatplotlib          3.8.4\nnumpy               1.26.4\nscipy               1.13.0\nsession_info        1.0.0\nsimulate            NA\ntqdm                4.66.4\n-----\n\n\nClick to view modules imported as dependencies\nPIL                         10.3.0\nanyio                       NA\nappnope                     0.1.3\nasttokens                   NA\nattr                        23.1.0\nattrs                       23.1.0\nbabel                       2.11.0\nbrotli                      1.0.9\ncertifi                     2024.02.02\ncffi                        1.16.0\ncharset_normalizer          2.0.4\ncolorama                    0.4.6\ncomm                        0.2.1\ncycler                      0.10.0\ncython_runtime              NA\ndateutil                    2.8.2\ndebugpy                     1.6.7\ndecorator                   5.1.1\ndefusedxml                  0.7.1\nexecuting                   0.8.3\nfastjsonschema              NA\nidna                        3.7\nipykernel                   6.28.0\njedi                        0.18.1\njinja2                      3.1.3\njson5                       NA\njsonschema                  4.19.2\njsonschema_specifications   NA\njupyter_events              0.8.0\njupyter_server              2.10.0\njupyterlab_server           2.25.1\nkiwisolver                  1.4.4\nmarkupsafe                  2.1.3\nmatplotlib_inline           0.1.6\nmpl_toolkits                NA\nnbformat                    5.9.2\noverrides                   NA\npackaging                   23.2\npandas                      2.2.2\nparso                       0.8.3\npexpect                     4.8.0\npkg_resources               NA\nplatformdirs                3.10.0\nprometheus_client           NA\nprompt_toolkit              3.0.43\npsutil                      5.9.0\nptyprocess                  0.7.0\npure_eval                   0.2.2\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.9.5\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.15.1\npyparsing                   3.0.9\npythonjsonlogger            NA\npytz                        2024.1\nreferencing                 NA\nrequests                    2.31.0\nrfc3339_validator           0.1.4\nrfc3986_validator           0.1.1\nrpds                        NA\nsend2trash                  NA\nsix                         1.16.0\nsniffio                     1.3.0\nsocks                       1.7.1\nstack_data                  0.2.0\ntornado                     6.3.3\ntraitlets                   5.7.1\nunicodedata2                NA\nurllib3                     2.2.1\nwcwidth                     0.2.5\nwebsocket                   1.8.0\nyaml                        6.0.1\nzmq                         25.1.2\n\n \n-----\nIPython             8.20.0\njupyter_client      8.6.0\njupyter_core        5.5.0\njupyterlab          4.0.11\nnotebook            7.0.8\n-----\nPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:54:21) [Clang 16.0.6 ]\nmacOS-13.6.1-arm64-arm-64bit\n-----\nSession information updated at 2024-08-02 15:24"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hierarchy",
    "section": "",
    "text": "Welcome to hierarchy, a blog about using regression, causal inference, machine learning, and hierarchical (multilevel) modeling to distinguish signal from noise in data; usually by thinking about the data generating process.\nI graduated with my PhD from Columbia University in 2022, where the focus of my dissertation was the use of computational statistics to optimize experimental design of nonlinear, hierarchical models. Most of the work I do involves Python {TensorFlow, PyTorch, PyMC, Scipy}, R {brms, cmdstanr, tidyverse}, Stan (with BridgeStan), and a bit of Julia to answer the questions I have about my data. I’m passionate about statistical modeling to quantify uncertainty, causal inference, stochastic processes, game theory, and scientific ML.\nFind me here, or send me an email!"
  },
  {
    "objectID": "index.html#about-hierarchy",
    "href": "index.html#about-hierarchy",
    "title": "hierarchy",
    "section": "",
    "text": "Welcome to hierarchy, a blog about using regression, causal inference, machine learning, and hierarchical (multilevel) modeling to distinguish signal from noise in data; usually by thinking about the data generating process.\nI graduated with my PhD from Columbia University in 2022, where the focus of my dissertation was the use of computational statistics to optimize experimental design of nonlinear, hierarchical models. Most of the work I do involves Python {TensorFlow, PyTorch, PyMC, Scipy}, R {brms, cmdstanr, tidyverse}, Stan (with BridgeStan), and a bit of Julia to answer the questions I have about my data. I’m passionate about statistical modeling to quantify uncertainty, causal inference, stochastic processes, game theory, and scientific ML.\nFind me here, or send me an email!"
  }
]